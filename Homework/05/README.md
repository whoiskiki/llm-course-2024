# Домашнее задание 1: "Введение в языковое моделирование"

## Описание

Цель данного домашнего задания — с помощью RLHF обучить LLM генерировать негативные отзывы. Задание состоит из 2х этапов - обучение reward-модели и PPO

Само задание необходимо выполнять в Jupyter-ноутбуке, код из которого нужно скопировать в файлы в `sctipts` и запустить тесты. Также в ноутбуке есть `assert`-ы для вашего удобства. Подробности и описание необходимых функций можно найти в Jupyter-ноутбуке и в папке `scripts`

Задание будет считаться выполненным, если будут пройдены все unit-тесты и приложен Jupyter-ноутбук с логами обученных моделей. Тесты расположены в папке `tests` и запускаются командой:

```bash
python3 -m unittest --verbose tests/*.py
```


Для успешного выполнения задания, необходимо:

1. Реализовать класс `IMDBPairwiseDataset`
2. Обучить reward-модель
3. Реализовать функцию `compute_reward`
4. Реализовать функцию `eval_reward_model`
5. Реализовать функцию `generate_with_reward_guidance`
6. Дописать пару строчек кода в цикле обучения PPO
7. Обучить итоговую модель с помощью PPO


## Структура проекта

- **`scripts`** — файлы с кодом, который вам нужно дописать
- **`tests`** — unit-тесты.
- **`alignment_homework.ipynb`** - ноутбук, в котором представлен пайплайн обучения модели с использованием написанной вами кодовой базы. Этот ноутбук поможет вам визуализировать процесс обучения и результат генерации текста.


## Зависимости

+ Python 3.9

Установить зависимости можно из файла `requirement.txt`. Рекомендуется делать это в отдельной `venv`. Пример для Linux/MacOS:

```bash
python3 -m venv venv-hw-05
source venv-hw-05/bin/activate
python3 -m pip install -r requirements.txt
```
