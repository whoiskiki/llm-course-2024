{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения должны быть реализованы в файле *Homework/02/solution.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1: реализация self-attention(2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании вам предстоит самостоятельно реализовать механизм self-attention(пока не multihead-версию). Суть проста:\n",
    "- даны 3 тензора $ Q, K, V $ с размерами $ (BatchSize, SeqLength, HiddenDim) $\n",
    "- необходимо вычислить attention scores, применив softmax к результату $ \\frac{QK^{T}}{\\sqrt(HiddenDim)} $\n",
    "- вычислить attention output, умножив V на attention scores\n",
    "\n",
    "Входная и выходная размерности совпадают!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение реализуется в функции compute_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2: реализация multihead self-attention(3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обобщим предыдущий подход на случай, когда мы работаем с несколькими головами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- даны 3 тензора $Q, K, V$ с размерами $ (BatchSize, NHeads, SeqLength, DimPerHead) $ и квадратная матрица $O$ размера $ (NHeads * DimPerHead, NHeads * DimPerHead) $\n",
    "- нунжно посчитать self-attention для каждой головы\n",
    "- сконкатенировать результаты предыдущего пункта в один тензор\n",
    "- применить к предыдущему пункту матрицу O\n",
    "\n",
    "Входная и выходная размерности совпадают!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение реализуется в функции compute_multihead_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3: реализация RoPE(Rotary Position Embedding)(5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На лекции мы обсудили один из вариантов внедрения позиционной информации в представление токена- аддитивный, в рамках которого к эмбеддингу каждого токена $ x_{i} $ прибавляется позиционный эмбеддинг $ p_{i} $, соответствующий позиции этого токена. Такой эмбеддинг называется абсолютным, т.к. при его вычислении мы работаем с фиксированной позицией токена(проще говоря, при вычислении мы учитываем только один индекс, связанный с входной последовательностью). Затем оба эмбеддинга можно использовать при вычислении query/key/value-векторов:\n",
    "$$ f_{\\{q, k, v\\}}(x_{i}, i) = W_{\\{q, k, v\\}} \\cdot (x_{i} + p_{i}) $$\n",
    "\n",
    "В этом задании вам предлагается реализовать другой сценарий внедрения позиционной информации в представление токена- RoPE, который характеризуется:\n",
    "- мультипликативностью(вместо сложения эмбеддинга токена и вектора его позиционного представления мы будем умножать эмбеддинг токена на некую матрицу)\n",
    "- относительностью(вместо фиксированной позиции токена мы будем работать с двумя индексами i, j, которые будут определять расстояние между токенами под соответствующими индексами)\n",
    "\n",
    "При использовании данного подхода модели быстрее сходятся и дают лучшее качество в практических задачах.\n",
    "\n",
    "## Как это работает\n",
    "Вспомним, что в механизме attention мы считаем сходство между токенами i и j следующим образом: пусть $ q_{i} = f_{q}(x_{i}, i)$ и $ k_{j}=f_{k}(x_{i}, j)$. Тогда $$ sim(i, j) = <f_{q}(x_{i}, i), f_{k}(x_{i}, j)> $$ Мы хотим найти функцию $ g $  такую, что $$ <f_{q}(x_{i}, i), f_{k}(x_{j}, j)>  = g(x_{i}, x_{j}, i-j),$$ т.е. такую, чтобы при подсчёте она учитывала только НЕпозиционные эмбеддинги для каждого токена и расстояние между этими двумя токенами, но при этом её значение вычислялось через скалярное произведение 2 векторов. Если размерность наших векторов- 2, то нашу задачу можно решить через матрицу вращения. Пусть $$ f_{q}(x_{m}, m) = \\begin{pmatrix} cos(m \\theta) & -sin(m \\theta) \\\\ sin(m \\theta) & cos(m \\theta) \\end{pmatrix} \\begin{pmatrix} W_{1,1} & W_{1,2} \\\\ W_{2,1} & W_{2,2} \\end{pmatrix} \\begin{pmatrix} x_{1}^{m} \\\\ x_{2}^{m} \\end{pmatrix} = R_{m} W x_{m},$$ где $R_{m}$- матрица вращения. Тогда $$ <f_{q}(x_{i}, i), f_{k}(x_{j}, j)> = (R_{i} W x_{i})^{T}(R_{j} W x_{j}) = x_{i}^{T}W_{q}R_{i-j}W_{k}x_{j} = g(x_{i}, x_{j}, i-j)$$\n",
    "\n",
    "Обобщение решения задачи в многомерном будет работать так:\n",
    "1. все наши векторы имеют чётную размерность d\n",
    "2. матрица вращения будет иметь вид"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rotation_matrix](data/rotation_matrix.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последняя деталь пазла- это определение вычисления $ \\theta_{i} $ в матрице вращения: $$ \\theta_{i} = 10000 ^ {-2(i-1) /d}, i \\in {1, 2, ..., d / 2} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша задача\n",
    "Реализовать добавление позиционной информации с помощью RoPE. В нашем случае это будет означать, что каждый вектор $ x_{m} $ будет умножаться на матрицу $ R_{\\theta, m}^{d} $. На вход вашему решению будет приходить тензор $ x $ размера $ (BatchSize, SeqLength, NHeads, DimPerHead) .$ За более подробной информацией по интуиции подхода и деталям реализации можно и нужно обращаться к [статье](https://arxiv.org/abs/2104.09864).\n",
    "\n",
    "Входная и выходная размерности совпадают!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение реализуется в функции compute_rotary_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
